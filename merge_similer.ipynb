{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec7d6cd1-de49-45c7-99f8-97b7b6895eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\gaura\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "✅ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# SECTION 1: IMPORTS AND SETUP\n",
    "# ==========================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML Libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Utilities\n",
    "import math\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94d779b2-cbb7-45e7-8f94-6512b2b9ca98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset created with 8000 complaints\n",
      "   complaint_id                                 complaint   latitude  \\\n",
      "0             1    Water tank in my street is overflowing  29.354970   \n",
      "1             2                  Hydrant is leaking badly  29.355020   \n",
      "2             3  Drinking water pipeline is leaking badly  29.399063   \n",
      "3             4                  Hydrant is leaking badly  29.355014   \n",
      "4             5      The tap water is muddy and not clean  29.390032   \n",
      "\n",
      "   longitude category  \n",
      "0  79.481963    Water  \n",
      "1  79.482123    Water  \n",
      "2  79.421095    Water  \n",
      "3  79.482024    Water  \n",
      "4  79.459929    Water  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Your original complaint templates\n",
    "complaint_templates = {\n",
    "    # ... (your templates remain the same) ...\n",
    "    \"Water\": [\n",
    "        \"No water supply since morning in my area\",\n",
    "        \"Drinking water pipeline is leaking badly\",\n",
    "        \"Water supply is irregular for the last 3 days\",\n",
    "        \"The tap water is muddy and not clean\",\n",
    "        \"Water tank in my street is overflowing\",\n",
    "        \"Broken water line on main street\",\n",
    "        \"Hydrant is leaking badly\"\n",
    "    ],\n",
    "    \"Electricity\": [\n",
    "        \"Power cut in my area for more than 2 hours\",\n",
    "        \"Street lights are not working for 3 days\",\n",
    "        \"Voltage fluctuation is damaging appliances\",\n",
    "        \"Complete blackout in our building since last night\",\n",
    "        \"Transformer making loud noise near my house\",\n",
    "        \"Wires are sparking near the school\",\n",
    "        \"A power pole is leaning dangerously\"\n",
    "    ],\n",
    "    \"Garbage\": [\n",
    "        \"Garbage has not been collected for a week\",\n",
    "        \"Overflowing dustbins causing foul smell\",\n",
    "        \"Stray dogs scattering garbage everywhere\",\n",
    "        \"Waste burning near my street, creating smoke\",\n",
    "        \"Garbage pile blocking the footpath\"\n",
    "    ],\n",
    "    \"Road\": [\n",
    "        \"Large pothole near my house, dangerous for bikers\",\n",
    "        \"Broken road causing traffic jam daily\",\n",
    "        \"Speed breakers are too high and damaging vehicles\",\n",
    "        \"Construction material lying on the road\",\n",
    "        \"Rainwater logged on road making it slippery\",\n",
    "        \"Road has large cracks and needs repair\",\n",
    "        \"Fallen tree blocking the street\"\n",
    "    ],\n",
    "    \"Parking\": [\n",
    "        \"Cars are being parked illegally blocking my gate\",\n",
    "        \"Too many vehicles parked on footpath\",\n",
    "        \"No space left in residential parking area\",\n",
    "        \"Trucks parked on narrow road blocking way\",\n",
    "        \"People are parking on both sides of the road\"\n",
    "    ],\n",
    "    \"Drainage\": [\n",
    "        \"Drainage water overflowing on the street\",\n",
    "        \"Manhole cover is missing, dangerous for children\",\n",
    "        \"Blocked drainage causing bad smell\",\n",
    "        \"Sewage water mixing with drinking water\",\n",
    "        \"Drainage water collected in front of my house\"\n",
    "    ],\n",
    "    \"Fire\": [\n",
    "        \"Fire broke out in a shop nearby, need urgent help\",\n",
    "        \"Smoke coming from an apartment, possible fire\",\n",
    "        \"Small fire in garbage dump spreading fast\",\n",
    "        \"Transformer caught fire near main road\",\n",
    "        \"Short circuit caused fire in building basement\"\n",
    "    ],\n",
    "    \"Other\": [\n",
    "        \"Too many stray dogs chasing people\",\n",
    "        \"Loud construction work disturbing at night\",\n",
    "        \"Street flooded after yesterday's rain\",\n",
    "        \"Tree fallen on road blocking traffic\",\n",
    "        \"Unauthorized construction blocking pathway\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# A wider, more realistic set of hotspots for a larger city area\n",
    "category_hotspots = {\n",
    "    \"Water\": [{'lat': 29.390, 'lon': 79.460}, {'lat': 29.355, 'lon': 79.482}, {'lat': 29.399, 'lon': 79.421}],\n",
    "    \"Electricity\": [{'lat': 29.385, 'lon': 79.450}, {'lat': 29.361, 'lon': 79.495}, {'lat': 29.390, 'lon': 79.435}],\n",
    "    \"Garbage\": [{'lat': 29.395, 'lon': 79.455}, {'lat': 29.378, 'lon': 79.470}, {'lat': 29.385, 'lon': 79.442}],\n",
    "    \"Road\": [{'lat': 29.401, 'lon': 79.452}, {'lat': 29.370, 'lon': 79.465}, {'lat': 29.389, 'lon': 79.480}],\n",
    "    \"Parking\": [{'lat': 29.388, 'lon': 79.458}, {'lat': 29.350, 'lon': 79.440}, {'lat': 29.405, 'lon': 79.455}],\n",
    "    \"Drainage\": [{'lat': 29.392, 'lon': 79.463}, {'lat': 29.365, 'lon': 79.445}, {'lat': 29.410, 'lon': 79.470}],\n",
    "    \"Fire\": [{'lat': 29.380, 'lon': 79.457}, {'lat': 29.393, 'lon': 79.459}, {'lat': 29.372, 'lon': 79.488}],\n",
    "    \"Other\": [{'lat': 29.397, 'lon': 79.451}, {'lat': 29.382, 'lon': 79.468}, {'lat': 29.369, 'lon': 79.475}]\n",
    "}\n",
    "\n",
    "# New function to check if a point is within the Nainital district bounds\n",
    "def is_within_bounds(lat, lon):\n",
    "    min_lat, max_lat = 28.9755, 29.6126\n",
    "    min_lon, max_lon = 78.8531, 79.9731\n",
    "    return min_lat <= lat <= max_lat and min_lon <= lon <= max_lon\n",
    "\n",
    "def generate_complaint(category, hotspots, radius_meters):\n",
    "    hotspot = random.choice(hotspots[category])\n",
    "    hotspot_lat = hotspot['lat']\n",
    "    hotspot_lon = hotspot['lon']\n",
    "    complaint_text = random.choice(complaint_templates[category])\n",
    "    \n",
    "    deg_lat_per_meter = 1 / 111111 \n",
    "    deg_lon_per_meter = 1 / (111111 * np.cos(np.radians(hotspot_lat)))\n",
    "    \n",
    "    random_dist_meters = random.uniform(0, radius_meters)\n",
    "    random_angle = random.uniform(0, 2 * np.pi)\n",
    "    \n",
    "    delta_lat = random_dist_meters * np.cos(random_angle) * deg_lat_per_meter\n",
    "    delta_lon = random_dist_meters * np.sin(random_angle) * deg_lon_per_meter\n",
    "    \n",
    "    new_lat = hotspot_lat + delta_lat\n",
    "    new_lon = hotspot_lon + delta_lon\n",
    "    \n",
    "    # Check if the new location is within bounds, if not, try again\n",
    "    if is_within_bounds(new_lat, new_lon):\n",
    "        return [new_lat, new_lon, complaint_text, category]\n",
    "    else:\n",
    "        # Recursively call the function until a valid point is generated\n",
    "        return generate_complaint(category, hotspots, radius_for_clustering)\n",
    "\n",
    "# Generate dataset\n",
    "data = []\n",
    "complaint_id = 1\n",
    "distribution = {\n",
    "    \"Water\": 1000, \"Electricity\": 1000, \"Garbage\": 1000,\n",
    "    \"Road\": 1000, \"Parking\": 1000, \"Drainage\": 1000,\n",
    "    \"Fire\": 1000, \"Other\": 1000\n",
    "}\n",
    "radius_for_clustering = 30\n",
    "\n",
    "for category, count in distribution.items():\n",
    "    for _ in range(count):\n",
    "        lat, lon, complaint_text, cat = generate_complaint(\n",
    "            category, category_hotspots, radius_for_clustering\n",
    "        )\n",
    "        data.append([complaint_id, complaint_text, lat, lon, cat])\n",
    "        complaint_id += 1\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"complaint_id\", \"complaint\", \"latitude\", \"longitude\", \"category\"])\n",
    "print(f\"✅ Dataset created with {len(df)} complaints\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "777d6075-751e-4443-8074-e4a40457d863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ComplaintDBSCANClustering:\n",
    "    def __init__(self, eps_distance=0.05, min_samples=3, model_name='sentence-transformers/all-MiniLM-L6-v2'):\n",
    "        \"\"\"\n",
    "        Initializes the DBSCAN clustering system.\n",
    "\n",
    "        Args:\n",
    "            eps_distance (float): The maximum distance between two samples for them to be considered\n",
    "                                 as in the same neighborhood. This is a combined metric of text and GPS distance.\n",
    "            min_samples (int): The number of samples in a neighborhood for a point to be considered a core point.\n",
    "        \"\"\"\n",
    "        self.eps_distance = eps_distance\n",
    "        self.min_samples = min_samples\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        print(\"🚀 Loading Sentence-BERT model...\")\n",
    "        self.sentence_model = SentenceTransformer(model_name)\n",
    "        print(\"✅ Model loaded successfully!\")\n",
    "        \n",
    "        self.clusters = None\n",
    "        self.group_id_counter = 1\n",
    "\n",
    "    def create_features(self, df):\n",
    "        \"\"\"\n",
    "        Creates a combined feature vector from text embeddings and scaled GPS data.\n",
    "        \"\"\"\n",
    "        print(\"📊 Creating text embeddings...\")\n",
    "        text_embeddings = self.sentence_model.encode(df['complaint'].tolist(), show_progress_bar=True)\n",
    "        print(\"✅ Text embeddings created!\")\n",
    "        \n",
    "        # Scale the GPS coordinates to a similar range as the text embeddings\n",
    "        print(\"📍 Scaling GPS coordinates...\")\n",
    "        gps_coords = df[['latitude', 'longitude']].values\n",
    "        scaler = StandardScaler()\n",
    "        scaled_gps = scaler.fit_transform(gps_coords)\n",
    "        print(\"✅ GPS coordinates scaled!\")\n",
    "\n",
    "        # Combine the features into a single array\n",
    "        combined_features = np.hstack((text_embeddings, scaled_gps))\n",
    "        return combined_features\n",
    "\n",
    "    def run_clustering(self, combined_features):\n",
    "        \"\"\"\n",
    "        Runs the DBSCAN clustering algorithm on the combined features.\n",
    "        \"\"\"\n",
    "        print(f\"\\n🧠 Running DBSCAN clustering with eps={self.eps_distance}, min_samples={self.min_samples}...\")\n",
    "        \n",
    "        dbscan = DBSCAN(eps=self.eps_distance, min_samples=self.min_samples, metric='euclidean')\n",
    "        clusters = dbscan.fit_predict(combined_features)\n",
    "        \n",
    "        print(\"✅ Clustering complete!\")\n",
    "        return clusters\n",
    "\n",
    "    def assign_groups_to_df(self, df, clusters):\n",
    "        \"\"\"\n",
    "        Assigns the cluster labels to the DataFrame and creates complaint groups.\n",
    "        \"\"\"\n",
    "        df['cluster_id'] = clusters\n",
    "        self.clusters = clusters\n",
    "        \n",
    "        self.complaint_groups = {}\n",
    "        unique_clusters = set(clusters)\n",
    "        \n",
    "        print(\"📂 Assigning complaints to groups...\")\n",
    "        for cluster_id in unique_clusters:\n",
    "            if cluster_id == -1:\n",
    "                # -1 cluster ID means noise (unclustered complaints)\n",
    "                continue\n",
    "            \n",
    "            group_df = df[df['cluster_id'] == cluster_id]\n",
    "            group_id = f\"G{str(self.group_id_counter).zfill(3)}\"\n",
    "            self.group_id_counter += 1\n",
    "            \n",
    "            # Create a list of complaints in the group\n",
    "            complaints_list = group_df.to_dict('records')\n",
    "            \n",
    "            # Calculate the center of the group\n",
    "            lats = group_df['latitude'].values\n",
    "            lons = group_df['longitude'].values\n",
    "\n",
    "            self.complaint_groups[group_id] = {\n",
    "                'group_id': group_id,\n",
    "                'priority': len(complaints_list),\n",
    "                'complaints': complaints_list,\n",
    "                'center_latitude': np.mean(lats),\n",
    "                'center_longitude': np.mean(lons),\n",
    "                'category': complaints_list[0]['category'] # Take the category of the first complaint\n",
    "            }\n",
    "        print(\"✅ Groups assigned!\")\n",
    "\n",
    "    def get_high_priority_groups(self, min_priority=3):\n",
    "        \"\"\"Get groups with priority >= min_priority.\"\"\"\n",
    "        high_priority = {\n",
    "            group_id: data for group_id, data in self.complaint_groups.items() \n",
    "            if data['priority'] >= min_priority\n",
    "        }\n",
    "        return high_priority\n",
    "\n",
    "    def save_model_package(self, filename='complaint_clustering_model.pkl'):\n",
    "        \"\"\"Save the processed groups and other data for backend integration.\"\"\"\n",
    "        model_package = {\n",
    "            'complaint_groups': self.complaint_groups,\n",
    "            'config': {\n",
    "                'eps_distance': self.eps_distance,\n",
    "                'min_samples': self.min_samples,\n",
    "                'model_name': self.model_name\n",
    "            },\n",
    "            'created_at': datetime.now().isoformat(),\n",
    "            'total_groups': len(self.complaint_groups),\n",
    "            'total_complaints': len(self.clusters)\n",
    "        }\n",
    "        \n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(model_package, f)\n",
    "        \n",
    "        print(f\"✅ Model package saved as {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba63e705-7043-4785-831e-3e70dea1f3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "  Running New DBSCAN Clustering Model     \n",
      "==========================================\n",
      "🚀 Loading Sentence-BERT model...\n",
      "✅ Model loaded successfully!\n",
      "📊 Creating text embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|███████████████████████████████████████████████████████████████████████| 250/250 [00:08<00:00, 29.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Text embeddings created!\n",
      "📍 Scaling GPS coordinates...\n",
      "✅ GPS coordinates scaled!\n",
      "\n",
      "🧠 Running DBSCAN clustering with eps=0.5, min_samples=3...\n",
      "✅ Clustering complete!\n",
      "📂 Assigning complaints to groups...\n",
      "✅ Groups assigned!\n",
      "\n",
      "📊 CLUSTERING RESULTS:\n",
      "Total Clusters Found: 138\n",
      "Noise Points (unclustered): 0\n",
      "Complaints in Clusters: 8000\n",
      "High Priority Groups (≥3 complaints): 138\n",
      "✅ Model package saved as complaint_clustering_model.pkl\n",
      "\n",
      "🎉 DBSCAN clustering is complete and files are ready for the backend.\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'df' is your DataFrame from the previous data generation step\n",
    "print(\"==========================================\")\n",
    "print(\"  Running New DBSCAN Clustering Model     \")\n",
    "print(\"==========================================\")\n",
    "\n",
    "# Initialize the new clustering system\n",
    "clustering_system = ComplaintDBSCANClustering(\n",
    "    eps_distance=0.5,  # You may need to tune this value\n",
    "    min_samples=3      # A group needs at least 3 complaints to be considered a cluster\n",
    ")\n",
    "\n",
    "# Create combined features\n",
    "combined_features = clustering_system.create_features(df)\n",
    "\n",
    "# Run the clustering\n",
    "clusters = clustering_system.run_clustering(combined_features)\n",
    "\n",
    "# Assign the results back to the DataFrame\n",
    "clustering_system.assign_groups_to_df(df.copy(), clusters)\n",
    "\n",
    "# Print a summary of the results\n",
    "print(f\"\\n📊 CLUSTERING RESULTS:\")\n",
    "total_groups = len(clustering_system.complaint_groups)\n",
    "print(f\"Total Clusters Found: {total_groups}\")\n",
    "noise_points = list(clusters).count(-1)\n",
    "print(f\"Noise Points (unclustered): {noise_points}\")\n",
    "print(f\"Complaints in Clusters: {len(df) - noise_points}\")\n",
    "\n",
    "# Get and print high-priority groups\n",
    "high_priority_groups = clustering_system.get_high_priority_groups(min_priority=3)\n",
    "print(f\"High Priority Groups (≥3 complaints): {len(high_priority_groups)}\")\n",
    "\n",
    "# Save the results\n",
    "clustering_system.save_model_package()\n",
    "\n",
    "print(\"\\n🎉 DBSCAN clustering is complete and files are ready for the backend.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d119db3-c2e6-4240-b066-10ef960f1f6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Complaint Clustering Model Silhouette Score: 0.9863\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have the 'combined_features' and 'clusters' from your DBSCAN model\n",
    "\n",
    "# Get rid of noise points (-1) before calculating the score\n",
    "valid_indices = clusters != -1\n",
    "valid_features = combined_features[valid_indices]\n",
    "valid_clusters = clusters[valid_indices]\n",
    "\n",
    "if len(np.unique(valid_clusters)) > 1:\n",
    "    score = silhouette_score(valid_features, valid_clusters)\n",
    "    print(f\"\\n📊 Complaint Clustering Model Silhouette Score: {score:.4f}\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Not enough clusters to calculate Silhouette Score.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d621660a-ca13-4795-ae09-cfc9372a1e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Loading Complaint DBSCAN Clustering Model...\n",
      "✅ Backend Model Ready! You can now use the `process_new_complaint` function.\n",
      "\n",
      "============================================================\n",
      "FAST TESTING PHASE\n",
      "============================================================\n",
      "Result 1: {'action': 'merged', 'group_id': 'G010', 'distance': np.float64(0.6623924859390872), 'total_complaints': 54}\n",
      "Result 2: {'action': 'merged', 'group_id': 'G010', 'distance': np.float64(0.5923695250521482), 'total_complaints': 54}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "# ==========================================\n",
    "# SECTION 1: LOAD YOUR TRAINED MODEL ONCE\n",
    "# ==========================================\n",
    "\n",
    "print(\"🚀 Loading Complaint DBSCAN Clustering Model...\")\n",
    "with open('complaint_clustering_model.pkl', 'rb') as f:\n",
    "    model_package = pickle.load(f)\n",
    "\n",
    "# The model package contains the processed groups\n",
    "complaint_groups = model_package['complaint_groups']\n",
    "\n",
    "# Load the Sentence-BERT model only once\n",
    "sentence_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Create a class that loads everything at startup\n",
    "# Create a class that loads everything at startup\n",
    "class RealtimeDBSCANProcessor:\n",
    "    def __init__(self, sentence_model, complaint_groups, eps_distance=0.5): # <-- Corrected init method\n",
    "        self.sentence_model = sentence_model\n",
    "        self.complaint_groups = complaint_groups\n",
    "        self.eps_distance = eps_distance\n",
    "\n",
    "    def process_new_complaint(self, new_complaint_text, new_lat, new_lon):\n",
    "        # Create embedding for the new complaint\n",
    "        new_embedding = self.sentence_model.encode([new_complaint_text]).reshape(1, -1)\n",
    "        \n",
    "        # We need to find the closest existing complaint group\n",
    "        min_distance = float('inf')\n",
    "        best_group_id = None\n",
    "        \n",
    "        for group_id, group_data in self.complaint_groups.items():\n",
    "            # Get the embedding for a representative complaint from the existing group\n",
    "            representative_complaint_text = group_data['complaints'][0]['complaint']\n",
    "            representative_embedding = self.sentence_model.encode([representative_complaint_text]).reshape(1, -1)\n",
    "            \n",
    "            # Combine text and GPS for distance calculation\n",
    "            combined_new = np.hstack((new_embedding, StandardScaler().fit_transform(np.array([[new_lat, new_lon]]))))\n",
    "            combined_rep = np.hstack((representative_embedding, StandardScaler().fit_transform(np.array([[group_data['center_latitude'], group_data['center_longitude']]]))))\n",
    "            \n",
    "            distance = np.linalg.norm(combined_new - combined_rep)\n",
    "\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                best_group_id = group_id\n",
    "\n",
    "        if min_distance <= self.eps_distance:\n",
    "            # Merged with an existing group\n",
    "            return {\n",
    "                'action': 'merged',\n",
    "                'group_id': best_group_id,\n",
    "                'distance': min_distance,\n",
    "                'total_complaints': len(self.complaint_groups[best_group_id]['complaints'])\n",
    "            }\n",
    "        else:\n",
    "            # New group\n",
    "            return {\n",
    "                'action': 'new_group',\n",
    "                'distance': min_distance,\n",
    "                'total_complaints': 1\n",
    "            }\n",
    "\n",
    "print(\"✅ Backend Model Ready! You can now use the `process_new_complaint` function.\")\n",
    "\n",
    "# ==========================================\n",
    "# SECTION 2: FAST REAL-TIME TESTING\n",
    "# ==========================================\n",
    "\n",
    "processor = RealtimeDBSCANProcessor(sentence_model, complaint_groups, eps_distance=0.7)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FAST TESTING PHASE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test 1: New complaint\n",
    "result1 = processor.process_new_complaint(\"Water is not coming since morning\", 29.391, 79.461)\n",
    "print(f\"Result 1: {result1}\")\n",
    "\n",
    "# Test 2: Similar complaint that should merge\n",
    "result2 = processor.process_new_complaint(\"No water supply in this area\", 29.390, 79.460)\n",
    "print(f\"Result 2: {result2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c83d86-7d97-4bb7-8d85-bd029b34a31c",
   "metadata": {},
   "source": [
    "# Red Zone Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3b33f9d-17aa-4bcd-9d84-07c11bfe7051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Running Red Zone Detection Model...\n",
      "✅ Red zone map data file saved as 'red_zone_map_data.json'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Assuming 'df' is your DataFrame from the previous data generation step\n",
    "\n",
    "# --- Red Zone Detector Class ---\n",
    "class RedZoneDetector:\n",
    "    def __init__(self, grid_size_meters=500):\n",
    "        self.grid_size_meters = grid_size_meters\n",
    "        self.grid_data = {}  # Stores data for each grid\n",
    "\n",
    "    def _get_grid_id(self, lat, lon):\n",
    "        \"\"\"Calculates a unique ID for a grid square based on its coordinates.\"\"\"\n",
    "        lat_per_meter = 1 / 111111\n",
    "        lon_per_meter = 1 / (111111 * np.cos(np.radians(lat)))\n",
    "        \n",
    "        grid_lat = int(lat / (self.grid_size_meters * lat_per_meter))\n",
    "        grid_lon = int(lon / (self.grid_size_meters * lon_per_meter))\n",
    "        \n",
    "        return f\"{grid_lat}_{grid_lon}\"\n",
    "\n",
    "    def assign_complaints_to_grids(self, complaints: pd.DataFrame):\n",
    "        \"\"\"Assigns each complaint to its corresponding grid square.\"\"\"\n",
    "        self.grid_data = {}\n",
    "        for _, row in complaints.iterrows():\n",
    "            grid_id = self._get_grid_id(row['latitude'], row['longitude'])\n",
    "            if grid_id not in self.grid_data:\n",
    "                self.grid_data[grid_id] = {'count': 0, 'complaints': []}\n",
    "            self.grid_data[grid_id]['count'] += 1\n",
    "            self.grid_data[grid_id]['complaints'].append(row.to_dict())\n",
    "\n",
    "    def get_map_data(self):\n",
    "        \"\"\"Prepares the data for Google Maps visualization with risk levels.\"\"\"\n",
    "        map_data = {'zones': []}\n",
    "        for grid_id, data in self.grid_data.items():\n",
    "            count = data['count']\n",
    "            \n",
    "            if count >= 50: risk, color = \"RED\", \"#FF0000\"\n",
    "            elif count >= 25: risk, color = \"ORANGE\", \"#FFA500\"\n",
    "            elif count >= 10: risk, color = \"YELLOW\", \"#FFFF00\"\n",
    "            else: risk, color = \"GREEN\", \"#00FF00\"\n",
    "            \n",
    "            if count > 0:\n",
    "                lats = [c['latitude'] for c in data['complaints']]\n",
    "                lons = [c['longitude'] for c in data['complaints']]\n",
    "                center_lat, center_lon = np.mean(lats), np.mean(lons)\n",
    "                map_data['zones'].append({\n",
    "                    'grid_id': grid_id, \n",
    "                    'complaint_count': count, \n",
    "                    'risk_level': risk,\n",
    "                    'color': color, \n",
    "                    'center_lat': center_lat, \n",
    "                    'center_lon': center_lon\n",
    "                })\n",
    "        return map_data\n",
    "\n",
    "# --- Execution ---\n",
    "print(\"🚀 Running Red Zone Detection Model...\")\n",
    "red_zone_detector = RedZoneDetector()\n",
    "red_zone_detector.assign_complaints_to_grids(df)\n",
    "map_data = red_zone_detector.get_map_data()\n",
    "\n",
    "with open('red_zone_map_data.json', 'w') as f:\n",
    "    json.dump(map_data, f, indent=4)\n",
    "print(f\"✅ Red zone map data file saved as 'red_zone_map_data.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbab292f-fa54-48ed-9994-6f5d1a868be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Running Red Zone Detection Model...\n",
      "✅ Red zone map data file saved as 'red_zone_map_data.json'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Red Zone Detector Class ---\n",
    "# This class contains the logic to divide the map into a grid and detect high-density areas.\n",
    "class RedZoneDetector:\n",
    "    def __init__(self, grid_size_meters=500):\n",
    "        self.grid_size_meters = grid_size_meters\n",
    "        self.grid_data = {}\n",
    "\n",
    "    def _get_grid_id(self, lat, lon):\n",
    "        \"\"\"Calculates a unique ID for a grid square based on its coordinates.\"\"\"\n",
    "        lat_per_meter = 1 / 111111\n",
    "        lon_per_meter = 1 / (111111 * np.cos(np.radians(lat)))\n",
    "        grid_lat = int(lat / (self.grid_size_meters * lat_per_meter))\n",
    "        grid_lon = int(lon / (self.grid_size_meters * lon_per_meter))\n",
    "        return f\"{grid_lat}_{grid_lon}\"\n",
    "\n",
    "    def assign_complaints_to_grids(self, complaints: pd.DataFrame):\n",
    "        \"\"\"Assigns each complaint to its corresponding grid square.\"\"\"\n",
    "        self.grid_data = {}\n",
    "        for _, row in complaints.iterrows():\n",
    "            grid_id = self._get_grid_id(row['latitude'], row['longitude'])\n",
    "            if grid_id not in self.grid_data:\n",
    "                self.grid_data[grid_id] = {'count': 0, 'complaints': []}\n",
    "            self.grid_data[grid_id]['count'] += 1\n",
    "            self.grid_data[grid_id]['complaints'].append(row.to_dict())\n",
    "\n",
    "    def get_map_data(self):\n",
    "        \"\"\"Prepares the data for Google Maps visualization with risk levels.\"\"\"\n",
    "        map_data = {'zones': []}\n",
    "        for grid_id, data in self.grid_data.items():\n",
    "            count = data['count']\n",
    "            if count >= 50: risk, color = \"RED\", \"#FF0000\"\n",
    "            elif count >= 25: risk, color = \"ORANGE\", \"#FFA500\"\n",
    "            elif count >= 10: risk, color = \"YELLOW\", \"#FFFF00\"\n",
    "            else: risk, color = \"GREEN\", \"#00FF00\"\n",
    "            if count > 0:\n",
    "                lats = [c['latitude'] for c in data['complaints']]\n",
    "                lons = [c['longitude'] for c in data['complaints']]\n",
    "                center_lat, center_lon = np.mean(lats), np.mean(lons)\n",
    "                map_data['zones'].append({\n",
    "                    'grid_id': grid_id,\n",
    "                    'complaint_count': count,\n",
    "                    'risk_level': risk,\n",
    "                    'color': color,\n",
    "                    'center_lat': center_lat,\n",
    "                    'center_lon': center_lon\n",
    "                })\n",
    "        return map_data\n",
    "\n",
    "# --- Execution ---\n",
    "# Assumes 'df' is a pandas DataFrame with complaint data\n",
    "print(\"🚀 Running Red Zone Detection Model...\")\n",
    "red_zone_detector = RedZoneDetector()\n",
    "red_zone_detector.assign_complaints_to_grids(df)\n",
    "map_data = red_zone_detector.get_map_data()\n",
    "\n",
    "with open('red_zone_map_data.json', 'w') as f:\n",
    "    json.dump(map_data, f, indent=4)\n",
    "print(f\"✅ Red zone map data file saved as 'red_zone_map_data.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b6fc57a-6287-43c2-aa18-e1097e0d6fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map saved to 'red_zone_map.html'. Open this file in your browser to view it.\n"
     ]
    }
   ],
   "source": [
    "import folium\n",
    "import json\n",
    "\n",
    "# Define the center of your map (Nainital coordinates)\n",
    "map_center = [29.3909, 79.4632]\n",
    "\n",
    "# Create a base map\n",
    "my_map = folium.Map(location=map_center, zoom_start=14)\n",
    "\n",
    "# Load the JSON data generated by your model\n",
    "with open('red_zone_map_data.json', 'r') as f:\n",
    "    map_data = json.load(f)\n",
    "\n",
    "# Loop through each zone and add a circle marker to the map\n",
    "for zone in map_data['zones']:\n",
    "    folium.Circle(\n",
    "        location=[zone['center_lat'], zone['center_lon']],\n",
    "        radius=300, # A 500-meter radius to match your grid size\n",
    "        color=zone['color'],\n",
    "        fill=True,\n",
    "        fillColor=zone['color'],\n",
    "        fillOpacity=0.5,\n",
    "        tooltip=f\"<b>Risk Level:</b> {zone['risk_level']}<br>\"\n",
    "                f\"<b>Complaints:</b> {zone['complaint_count']}<br>\"\n",
    "                f\"<b>Grid ID:</b> {zone['grid_id']}\"\n",
    "    ).add_to(my_map)\n",
    "\n",
    "# Add a simple legend\n",
    "legend_html = '''\n",
    "     <div style=\"position: fixed; \n",
    "                 bottom: 20px; left: 20px; width: 150px; height: 120px; \n",
    "                 border:2px solid grey; z-index:9999; font-size:14px;\n",
    "                 background-color:white; opacity:0.9;\">\n",
    "       &nbsp; <b>Risk Levels</b> <br>\n",
    "       &nbsp; <i style=\"background:#FF0000; display:inline-block; border:1px solid black; width:10px; height:10px; border-radius:50%;\"></i> Red Zone (50+)<br>\n",
    "       &nbsp; <i style=\"background:#FFA500; display:inline-block; border:1px solid black; width:10px; height:10px; border-radius:50%;\"></i> Orange Zone (25-49)<br>\n",
    "       &nbsp; <i style=\"background:#FFFF00; display:inline-block; border:1px solid black; width:10px; height:10px; border-radius:50%;\"></i> Yellow Zone (10-24)<br>\n",
    "       &nbsp; <i style=\"background:#00FF00; display:inline-block; border:1px solid black; width:10px; height:10px; border-radius:50%;\"></i> Green Zone (0-9)<br>\n",
    "     </div>\n",
    "     '''\n",
    "my_map.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "\n",
    "# Save the map to an HTML file\n",
    "my_map.save(\"red_zone_map.html\")\n",
    "print(\"Map saved to 'red_zone_map.html'. Open this file in your browser to view it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff84c81-5b2c-428f-a4a2-64d6a5a5f94c",
   "metadata": {},
   "source": [
    "# Priority prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b61fb89b-7fd3-4fc2-bc65-cc3108d67ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sample Labeled Dataset:\n",
      "                                       complaint category priority_label\n",
      "0         Water tank in my street is overflowing    Water           high\n",
      "1                       Hydrant is leaking badly    Water           high\n",
      "2       Drinking water pipeline is leaking badly    Water           high\n",
      "3                       Hydrant is leaking badly    Water           high\n",
      "4           The tap water is muddy and not clean    Water           high\n",
      "5       Drinking water pipeline is leaking badly    Water           high\n",
      "6       Drinking water pipeline is leaking badly    Water           high\n",
      "7       Drinking water pipeline is leaking badly    Water           high\n",
      "8  Water supply is irregular for the last 3 days    Water           high\n",
      "9               Broken water line on main street    Water           high\n",
      "\n",
      "📊 Priority Label Distribution:\n",
      "priority_label\n",
      "high    5000\n",
      "low     3000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create a new column to store the priority label\n",
    "df['priority_label'] = 'low'\n",
    "\n",
    "# Define high-priority categories\n",
    "high_priority_categories = ['Fire', 'Water', 'Electricity', 'Drainage', 'Road']\n",
    "\n",
    "# Loop through the DataFrame and assign 'high' priority based on the category\n",
    "for category in high_priority_categories:\n",
    "    df.loc[df['category'] == category, 'priority_label'] = 'high'\n",
    "\n",
    "# Display a sample of the new DataFrame\n",
    "print(\"✅ Sample Labeled Dataset:\")\n",
    "print(df[['complaint', 'category', 'priority_label']].head(10))\n",
    "\n",
    "# Print the final count of each priority to check the balance\n",
    "print(\"\\n📊 Priority Label Distribution:\")\n",
    "print(df['priority_label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d4420a5-fe6b-48a2-b7c1-24e4bb332f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All libraries loaded.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW  # <-- Correct Import from torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "print(\"✅ All libraries loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "009cf855-58dc-4639-b031-20d9cd4fca7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Loading BERT tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|████████████████████████████████████████████████████████████████| 8000/8000 [00:02<00:00, 3623.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data preparation complete!\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'df' is your DataFrame from the previous steps with 'complaint' and 'priority_label'\n",
    "# Map priority labels to integers (0 for low, 1 for high)\n",
    "priority_map = {'low': 0, 'high': 1}\n",
    "df['label'] = df['priority_label'].map(priority_map)\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "print(\"🚀 Loading BERT tokenizer...\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Tokenize all of the complaints in your dataset\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for complaint in tqdm(df.complaint.values, desc=\"Tokenizing\"):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        complaint,\n",
    "        add_special_tokens=True,\n",
    "        max_length=64, # Use the same fixed length for all sentences\n",
    "        truncation=True, # Explicitly truncate longer sequences\n",
    "        padding='max_length', # Explicitly pad shorter sequences\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(df.label.values)\n",
    "\n",
    "# Split data into training and validation sets (85% train, 15% val)\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(\n",
    "    input_ids, labels, random_state=42, test_size=0.15, stratify=labels\n",
    ")\n",
    "train_masks, validation_masks, _, _ = train_test_split(\n",
    "    attention_masks, labels, random_state=42, test_size=0.15, stratify=labels\n",
    ")\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "batch_size = 32\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "print(\"✅ Data preparation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2b347df-4331-4fae-a6aa-43a3c63605e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using device: cpu\n",
      "\n",
      "🚀 Starting model fine-tuning...\n",
      "======== Epoch 1 / 4 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 213/213 [17:38<00:00,  4.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Average training loss: 0.05\n",
      "\n",
      "  Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 38/38 [00:46<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Loss: 0.00\n",
      "======== Epoch 2 / 4 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 213/213 [18:25<00:00,  5.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Average training loss: 0.00\n",
      "\n",
      "  Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 38/38 [00:46<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Loss: 0.00\n",
      "======== Epoch 3 / 4 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 213/213 [17:39<00:00,  4.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Average training loss: 0.00\n",
      "\n",
      "  Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 38/38 [00:48<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Loss: 0.00\n",
      "======== Epoch 4 / 4 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 213/213 [20:55<00:00,  5.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Average training loss: 0.00\n",
      "\n",
      "  Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 38/38 [00:56<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Loss: 0.00\n",
      "\n",
      "✅ Fine-tuning complete!\n",
      "✅ Model saved to 'priority_prediction_model.pth'\n"
     ]
    }
   ],
   "source": [
    "# Load BertForSequenceClassification, the pre-trained BERT model with a single classification layer\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=2,    # The number of output labels (high and low)\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "\n",
    "# Tell the model we'll be using a GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "print(f\"✅ Using device: {device}\")\n",
    "\n",
    "# Optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "epochs = 4\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Training loop\n",
    "print(\"\\n🚀 Starting model fine-tuning...\")\n",
    "for epoch in range(epochs):\n",
    "    print(f\"======== Epoch {epoch + 1} / {epochs} ========\")\n",
    "    model.train() # Set the model to training mode\n",
    "\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
    "        b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        model.zero_grad() # Clear any previously calculated gradients\n",
    "\n",
    "        outputs = model(b_input_ids,\n",
    "                        token_type_ids=None,\n",
    "                        attention_mask=b_input_mask,\n",
    "                        labels=b_labels)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward() # Perform a backward pass to calculate gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Clip gradients to prevent exploding gradients\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"  Average training loss: {avg_train_loss:.2f}\")\n",
    "\n",
    "    # Validation step\n",
    "    print(\"\\n  Running Validation...\")\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "\n",
    "    total_eval_loss = 0\n",
    "    for batch in tqdm(validation_dataloader, desc=\"Validating\"):\n",
    "        b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids,\n",
    "                            token_type_ids=None,\n",
    "                            attention_mask=b_input_mask,\n",
    "                            labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        total_eval_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    print(f\"  Validation Loss: {avg_val_loss:.2f}\")\n",
    "\n",
    "print(\"\\n✅ Fine-tuning complete!\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "torch.save(model.state_dict(), 'priority_prediction_model.pth')\n",
    "print(\"✅ Model saved to 'priority_prediction_model.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319fea99-4e31-4a34-9d7c-849afb7a78f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
